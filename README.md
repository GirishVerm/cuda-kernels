# FlashAttention CUDA Kernels: Custom Implementation

[![CUDA](https://img.shields.io/badge/CUDA-11.0%2B-76B900?logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

A high-performance implementation of attention mechanisms for Large Language Models (LLMs) using custom CUDA kernels. This project demonstrates deep understanding of GPU architecture, memory optimization, and parallel programming techniques essential for efficient LLM inference.

## ğŸ¯ Project Overview

This project implements multiple versions of the attention mechanism with progressive optimizations:

1. **Naive Attention**: Baseline implementation for correctness verification
2. **Tiled Attention**: Optimized with shared memory and tiling strategies
3. **FlashAttention-style**: Advanced implementation with IO-awareness and recomputation

## ğŸš€ Key Features

- **Custom CUDA Kernels**: Hand-optimized kernels showcasing GPU programming expertise
- **PyTorch Integration**: Seamless integration as PyTorch custom operators
- **Comprehensive Benchmarking**: Detailed performance comparisons with metrics:
  - Throughput (TFLOPS)
  - Memory bandwidth utilization
  - Latency across different sequence lengths
  - GPU utilization
- **Profiling Suite**: Integration with NVIDIA Nsight tools
- **Performance Modeling**: Roofline analysis and bottleneck identification

## ğŸ“Š Performance Highlights

| Implementation | Relative Speedup | Memory Usage | Best Use Case |
|---------------|------------------|--------------|---------------|
| Naive | 1.0x (baseline) | 100% | Reference |
| Tiled | ~2-3x | ~60% | Medium sequences |
| FlashAttention | ~4-8x | ~20% | Long sequences |

*Benchmarked on NVIDIA A100 with sequence length 2048*

## ğŸ› ï¸ Technical Highlights

### GPU Optimization Techniques
- Memory coalescing for efficient global memory access
- Shared memory tiling to reduce memory bandwidth bottleneck
- Warp-level primitives for efficient reduction operations
- Register blocking to maximize compute throughput
- IO-aware design minimizing HBM reads/writes
- Kernel fusion to reduce memory round-trips

### Architecture Knowledge
- Understanding of CUDA memory hierarchy
- Occupancy optimization and resource management
- Bank conflict avoidance in shared memory
- Cooperative groups for flexible parallelism

## ğŸ“ Project Structure

```
llm-proj/
â”œâ”€â”€ csrc/                      # C++ and CUDA source files
â”‚   â”œâ”€â”€ attention_naive.cu     # Baseline attention kernel
â”‚   â”œâ”€â”€ attention_tiled.cu     # Tiled attention with shared memory
â”‚   â”œâ”€â”€ attention_flash.cu     # FlashAttention-style kernel
â”‚   â”œâ”€â”€ attention_kernels.h    # Kernel declarations
â”‚   â””â”€â”€ bindings.cpp           # PyTorch C++ bindings
â”œâ”€â”€ python/                    # Python package
â”‚   â”œâ”€â”€ attention_cuda/        # Main package
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ attention.py       # Python wrapper
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ benchmarks/            # Benchmarking scripts
â”‚   â”‚   â”œâ”€â”€ benchmark_attention.py
â”‚   â”‚   â”œâ”€â”€ compare_implementations.py
â”‚   â”‚   â””â”€â”€ visualize_results.py
â”‚   â””â”€â”€ profiling/             # Profiling utilities
â”‚       â”œâ”€â”€ profile_kernels.py
â”‚       â””â”€â”€ analyze_nsight.py
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ test_correctness.py
â”‚   â””â”€â”€ test_gradients.py
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ DESIGN.md             # Design decisions
â”‚   â”œâ”€â”€ OPTIMIZATION.md       # Optimization techniques
â”‚   â””â”€â”€ PROFILING.md          # Profiling guide
â”œâ”€â”€ setup.py                   # Build configuration
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Installation

### Prerequisites
- **Windows 10/11** with NVIDIA GPU (GTX 1060+ or RTX 20xx/30xx/40xx series)
- **CUDA Toolkit 11.8+** ([Download](https://developer.nvidia.com/cuda-downloads))
- **Visual Studio 2019 or 2022** with C++ tools ([Download](https://visualstudio.microsoft.com/downloads/))
- **Python 3.10 or 3.11** ([Download](https://www.python.org/downloads/))
- **PyTorch 2.0+ with CUDA** ([Install Guide](https://pytorch.org/get-started/locally/))

### Quick Start (Windows)

```cmd
# 1. Verify CUDA is working
nvidia-smi
nvcc --version

# 2. Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. Verify PyTorch CUDA
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"

# 4. Build the project
pip install -r requirements.txt
python setup.py develop

# 5. Run tests
pytest tests/ -v
```

### Automated Build

```cmd
# Run the automated build script
build.bat
```

## ğŸ’» Usage

### Basic Usage

```python
import torch
from attention_cuda import (
    attention_naive,
    attention_tiled, 
    attention_flash
)

# Input tensors
batch_size, num_heads, seq_len, head_dim = 2, 8, 512, 64
Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# Run optimized attention
output = attention_flash(Q, K, V)
```

### Benchmarking

```cmd
# Run comprehensive benchmarks
python python/benchmarks/benchmark_attention.py

# Compare with PyTorch native
python python/benchmarks/compare_implementations.py --seq-lengths 128,256,512,1024,2048

# Visualize results
python python/benchmarks/visualize_results.py
```

### Profiling

```cmd
# Profile with Nsight Compute (Windows)
ncu --set full --target-processes all python python/profiling/profile_kernels.py

# Profile with Nsight Systems
nsys profile --trace=cuda,nvtx python python/profiling/profile_kernels.py
```

## ğŸ“ˆ Benchmarking Results

### Throughput vs Sequence Length
![Throughput Comparison](docs/images/throughput_comparison.png)

### Memory Bandwidth Utilization
![Memory Bandwidth](docs/images/memory_bandwidth.png)

### Latency Analysis
![Latency](docs/images/latency_analysis.png)

## ğŸ§ª Testing

```cmd
# Run all tests
pytest tests/

# Test correctness against PyTorch
pytest tests/test_correctness.py -v

# Test gradient computation
pytest tests/test_gradients.py -v
```

## ğŸ“š Documentation

- [Design Decisions](docs/DESIGN.md) - Architecture and design choices
- [Optimization Techniques](docs/OPTIMIZATION.md) - Detailed optimization explanations
- [Profiling Guide](docs/PROFILING.md) - How to profile and analyze performance

## ğŸ“ Learning Resources

This project demonstrates concepts from:
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2 Paper](https://arxiv.org/abs/2307.08691)
- NVIDIA CUDA Programming Guide
- GPU Performance Optimization Techniques

## ğŸ—ï¸ Implementation Details

### Naive Attention
- Direct implementation of scaled dot-product attention
- O(NÂ²) memory complexity
- Serves as correctness baseline

### Tiled Attention
- Uses shared memory to cache Q, K, V tiles
- Reduces global memory accesses by ~3x
- Optimized for medium sequence lengths

### FlashAttention-style
- IO-aware implementation minimizing HBM access
- Online softmax computation to avoid materialization
- Recomputes attention scores in backward pass
- Achieves sub-linear memory complexity

## ğŸ”¬ Key Optimizations Explained

### 1. Memory Coalescing
```cuda
// Ensures adjacent threads access adjacent memory locations
float val = Q[threadIdx.x + blockIdx.x * blockDim.x];
```

### 2. Shared Memory Tiling
```cuda
__shared__ float Q_tile[BLOCK_SIZE][HEAD_DIM];
__shared__ float K_tile[BLOCK_SIZE][HEAD_DIM];
// Load tiles cooperatively to reduce global memory traffic
```

### 3. Warp-Level Reductions
```cuda
// Efficient reduction using warp shuffle operations
float sum = warpReduceSum(val);
```

## ğŸ¯ Skills Demonstrated

âœ… **CUDA Programming**: Custom kernel development with advanced optimizations  
âœ… **GPU Architecture**: Deep understanding of memory hierarchy and execution model  
âœ… **PyTorch Integration**: C++/CUDA extensions with autograd support  
âœ… **Performance Engineering**: Profiling, bottleneck analysis, and optimization  
âœ… **Deep Learning**: Understanding of transformer architecture and attention  
âœ… **Software Engineering**: Clean code, testing, documentation, and benchmarking  

## ğŸš§ Future Enhancements

- [ ] Multi-query attention (MQA) and Grouped-query attention (GQA)
- [ ] Support for causal masking and attention bias
- [ ] FP16/BF16 mixed precision support
- [ ] Multi-GPU support
- [ ] TensorRT plugin integration
- [ ] Triton backend support

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details

## ğŸ™ Acknowledgments

Inspired by:
- FlashAttention by Dao et al.
- PyTorch CUDA extension tutorials
- NVIDIA's CUTLASS library

## ğŸ“§ Contact

**Girish Verma**  
[GitHub](https://github.com/yourusername) | [LinkedIn](https://linkedin.com/in/yourprofile)

---

*This project was created to demonstrate expertise in GPU programming, performance optimization, and deep learning systems for LLM inference applications.*

