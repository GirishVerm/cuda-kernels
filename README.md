# FlashAttention CUDA Kernels: Custom Implementation

## Yes this README is written by AI, cause AI writes better READMEs than I do!

[![CUDA](https://img.shields.io/badge/CUDA-11.0%2B-76B900?logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-EE4C2C?logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

A high-performance implementation of attention mechanisms for Large Language Models (LLMs) using custom CUDA kernels. This project demonstrates deep understanding of GPU architecture, memory optimization, and parallel programming techniques essential for efficient LLM inference.

## Project Overview

This project implements multiple versions of the attention mechanism with progressive optimizations:

1. **Naive Attention**: Baseline implementation for correctness verification
2. **Tiled Attention**: Optimized with shared memory and tiling strategies
3. **FlashAttention-style**: Advanced implementation with IO-awareness and recomputation

## Key Features

- **Custom CUDA Kernels**: Hand-optimized kernels showcasing GPU programming expertise
- **PyTorch Integration**: Seamless integration as PyTorch custom operators
- **Comprehensive Benchmarking**: Detailed performance comparisons with metrics:
  - Throughput (TFLOPS)
  - Memory bandwidth utilization
  - Latency across different sequence lengths
  - GPU utilization
- **Profiling Suite**: Integration with NVIDIA Nsight tools
- **Performance Modeling**: Roofline analysis and bottleneck identification

## Performance Highlights

| Implementation | Relative Speedup | Memory Usage | Best Use Case |
|---------------|------------------|--------------|---------------|
| Naive | 1.0x (baseline) | 100% | Reference |
| Tiled | ~2-3x | ~60% | Medium sequences |
| FlashAttention | ~2-4x | ~20% | Long sequences |

**Tested on:** GTX 1650 Ti (4GB VRAM, Turing architecture, compute capability 7.5)
**Recommended:** Use sequence lengths up to 1024 with batch_size=1 or 2 due to memory constraints.

## Technical Highlights

### GPU Optimization Techniques
- Memory coalescing for efficient global memory access
- Shared memory tiling to reduce memory bandwidth bottleneck
- Warp-level primitives for efficient reduction operations
- Register blocking to maximize compute throughput
- IO-aware design minimizing HBM reads/writes
- Kernel fusion to reduce memory round-trips

### Architecture Knowledge
- Understanding of CUDA memory hierarchy
- Occupancy optimization and resource management
- Bank conflict avoidance in shared memory
- Cooperative groups for flexible parallelism

## Project Structure

```
llm-proj/
├── csrc/                      # C++ and CUDA source files
│   ├── attention_naive.cu     # Baseline attention kernel
│   ├── attention_tiled.cu     # Tiled attention with shared memory
│   ├── attention_flash.cu     # FlashAttention-style kernel
│   ├── attention_kernels.h    # Kernel declarations
│   └── bindings.cpp           # PyTorch C++ bindings
├── python/                    # Python package
│   ├── attention_cuda/        # Main package
│   │   ├── __init__.py
│   │   ├── attention.py       # Python wrapper
│   │   └── utils.py
│   ├── benchmarks/            # Benchmarking scripts
│   │   ├── benchmark_attention.py
│   │   ├── compare_implementations.py
│   │   └── visualize_results.py
│   └── profiling/             # Profiling utilities
│       ├── profile_kernels.py
│       └── analyze_nsight.py
├── tests/                     # Unit tests
│   ├── test_correctness.py
│   └── test_gradients.py
├── docs/                      # Documentation
│   ├── DESIGN.md             # Design decisions
│   ├── OPTIMIZATION.md       # Optimization techniques
│   └── PROFILING.md          # Profiling guide
├── setup.py                   # Build configuration
├── requirements.txt
└── README.md
```

## Installation

### Prerequisites
- **Windows 10/11** with NVIDIA GPU (GTX 1650 Ti or better)
- **CUDA Toolkit 11.8** ([Download](https://developer.nvidia.com/cuda-downloads))
- **Visual Studio 2019 or 2022** with C++ tools ([Download](https://visualstudio.microsoft.com/downloads/))
- **Python 3.10 or 3.11** ([Download](https://www.python.org/downloads/))
- **PyTorch 2.0+ with CUDA** ([Install Guide](https://pytorch.org/get-started/locally/))

**Note:** GTX 1650 Ti has 4GB VRAM - use smaller batch sizes and sequence lengths for testing.

### Quick Start (Windows)

```cmd
# 1. Verify CUDA is working
nvidia-smi
nvcc --version

# 2. Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 3. Verify PyTorch CUDA
python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"

# 4. Build the project
pip install -r requirements.txt
python setup.py develop

# 5. Run tests
pytest tests/ -v
```

### Automated Build

```cmd
# Run the automated build script
build.bat
```

## Usage

### Basic Usage

```python
import torch
from attention_cuda import (
    attention_naive,
    attention_tiled, 
    attention_flash
)

# Input tensors
batch_size, num_heads, seq_len, head_dim = 2, 8, 512, 64
Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# Run optimized attention
output = attention_flash(Q, K, V)
```

### Benchmarking

```cmd
# Run comprehensive benchmarks (adjusted for GTX 1650 Ti 4GB VRAM)
python python/benchmarks/benchmark_attention.py --batch-size 1 --seq-lengths 128,256,512,1024

# Compare with PyTorch native
python python/benchmarks/compare_implementations.py --batch-size 1 --seq-len 512

# Visualize results
python python/benchmarks/visualize_results.py
```

### Profiling

```cmd
# Profile with Nsight Compute (Windows)
ncu --set full --target-processes all python python/profiling/profile_kernels.py

# Profile with Nsight Systems
nsys profile --trace=cuda,nvtx python python/profiling/profile_kernels.py
```

## Benchmarking Results

### Throughput vs Sequence Length
![Throughput Comparison](docs/images/throughput_comparison.png)

### Memory Bandwidth Utilization
![Memory Bandwidth](docs/images/memory_bandwidth.png)

### Latency Analysis
![Latency](docs/images/latency_analysis.png)

## Testing

```cmd
# Run all tests
pytest tests/

# Test correctness against PyTorch
pytest tests/test_correctness.py -v

# Test gradient computation
pytest tests/test_gradients.py -v
```

## Documentation

- [Design Decisions](docs/DESIGN.md) - Architecture and design choices
- [Optimization Techniques](docs/OPTIMIZATION.md) - Detailed optimization explanations
- [Profiling Guide](docs/PROFILING.md) - How to profile and analyze performance

## Learning Resources

This project demonstrates concepts from:
- [FlashAttention Paper](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2 Paper](https://arxiv.org/abs/2307.08691)
- NVIDIA CUDA Programming Guide
- GPU Performance Optimization Techniques

## Implementation Details

### Naive Attention
- Direct implementation of scaled dot-product attention
- O(N²) memory complexity
- Serves as correctness baseline

### Tiled Attention
- Uses shared memory to cache Q, K, V tiles
- Reduces global memory accesses by ~3x
- Optimized for medium sequence lengths

### FlashAttention-style
- IO-aware implementation minimizing HBM access
- Online softmax computation to avoid materialization
- Recomputes attention scores in backward pass
- Achieves sub-linear memory complexity

## Key Optimizations Explained

### 1. Memory Coalescing
```cuda
// Ensures adjacent threads access adjacent memory locations
float val = Q[threadIdx.x + blockIdx.x * blockDim.x];
```

### 2. Shared Memory Tiling
```cuda
__shared__ float Q_tile[BLOCK_SIZE][HEAD_DIM];
__shared__ float K_tile[BLOCK_SIZE][HEAD_DIM];
// Load tiles cooperatively to reduce global memory traffic
```

### 3. Warp-Level Reductions
```cuda
// Efficient reduction using warp shuffle operations
float sum = warpReduceSum(val);
```

## Skills Demonstrated

**CUDA Programming**: Custom kernel development with advanced optimizations  
**GPU Architecture**: Deep understanding of memory hierarchy and execution model  
**PyTorch Integration**: C++/CUDA extensions with autograd support  
**Performance Engineering**: Profiling, bottleneck analysis, and optimization  
**Deep Learning**: Understanding of transformer architecture and attention  
**Software Engineering**: Clean code, testing, documentation, and benchmarking  

## Future Enhancements

- [ ] Multi-query attention (MQA) and Grouped-query attention (GQA)
- [ ] Support for causal masking and attention bias
- [ ] FP16/BF16 mixed precision support
- [ ] Multi-GPU support
- [ ] TensorRT plugin integration
- [ ] Triton backend support



## Acknowledgments

Inspired by:
- FlashAttention by Dao et al.
- PyTorch CUDA extension tutorials
- NVIDIA's CUTLASS library
---

*This project was created to demonstrate expertise in GPU programming, performance optimization, and deep learning systems for LLM inference applications.*

