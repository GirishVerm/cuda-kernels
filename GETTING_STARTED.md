# Getting Started

Quick guide to build, test, and run the CUDA attention kernels on Windows.

## Prerequisites

### Hardware Requirements
- **NVIDIA GPU** with compute capability 7.0+ (Volta/Turing/Ampere/Ada/Hopper)
  - Minimum: GTX 1060 (6GB VRAM)
  - Recommended: RTX 3060/3070/3080 or RTX 4070/4080
  - Ideal: RTX 4090 or A100

### Software Requirements

1. **Windows 10 or 11** (64-bit)

2. **NVIDIA GPU Drivers** (version 520+)
   - Download: https://www.nvidia.com/Download/index.aspx
   - Verify: Run `nvidia-smi` in Command Prompt

3. **CUDA Toolkit 11.8 or 12.1**
   - Download: https://developer.nvidia.com/cuda-downloads
   - Select: Windows â†’ x86_64 â†’ Your Windows version â†’ exe (local)
   - Verify: Run `nvcc --version` in Command Prompt

4. **Visual Studio 2019 or 2022** (Community Edition is free)
   - Download: https://visualstudio.microsoft.com/downloads/
   - During installation, select: **"Desktop development with C++"**
   - This provides the C++ compiler required by CUDA

5. **Python 3.10 or 3.11**
   - Download: https://www.python.org/downloads/
   - During installation: âœ… Check "Add Python to PATH"
   - Verify: Run `python --version`

6. **PyTorch with CUDA support**
   - For CUDA 11.8:
     ```cmd
     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
     ```
   - For CUDA 12.1:
     ```cmd
     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
     ```
   - Verify:
     ```cmd
     python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
     ```
   - Expected output: `CUDA Available: True` and your GPU name

## Installation

### Option 1: Automated Build (Recommended)

1. Open **Command Prompt** or **PowerShell**
2. Navigate to project directory:
   ```cmd
   cd C:\path\to\llm-proj
   ```
3. Run the build script:
   ```cmd
   build.bat
   ```

This will:
- Check prerequisites
- Install Python dependencies
- Build CUDA extensions
- Run tests

### Option 2: Manual Build

```cmd
# 1. Navigate to project
cd C:\path\to\llm-proj

# 2. Install Python dependencies
pip install -r requirements.txt

# 3. Build CUDA extensions (takes 2-5 minutes)
python setup.py develop

# 4. Verify installation
python -c "import attention_cuda; print('âœ“ Installation successful!')"

# 5. Run tests
pytest tests/ -v
```

## Quick Start

### 1. Verify Correctness

Compare implementations against PyTorch reference:

```cmd
python python/benchmarks/compare_implementations.py
```

**Expected output:**
```
CORRECTNESS COMPARISON
Implementation      Max Error       Status    
---------------------------------------------
naive              1.23e-04        âœ“ PASS
tiled              2.34e-04        âœ“ PASS
flash              5.67e-04        âœ“ PASS
```

### 2. Run Benchmarks

Benchmark all implementations across different sequence lengths:

```cmd
python python/benchmarks/benchmark_attention.py --seq-lengths 128,256,512,1024,2048
```

**Expected output (RTX 3080):**
```
Benchmarking: batch=2, heads=8, seq_len=512, head_dim=64

Benchmarking pytorch...
  Mean latency: 2.345 ms
  Throughput: 15.23 TFLOPS

Benchmarking naive...
  Mean latency: 4.567 ms
  Throughput: 7.82 TFLOPS

Benchmarking tiled...
  Mean latency: 1.956 ms
  Throughput: 18.23 TFLOPS

Benchmarking flash...
  Mean latency: 1.234 ms
  Throughput: 28.91 TFLOPS

flash speedup vs PyTorch: 1.90x
```

Results are saved to `benchmark_results.json`

### 3. Visualize Results

Generate plots from benchmark results:

```cmd
python python/benchmarks/visualize_results.py --input benchmark_results.json --output-dir benchmark_plots
```

This creates:
- `latency_comparison.png` - Bar chart comparing latencies
- `throughput_comparison.png` - Throughput vs sequence length
- `speedup_comparison.png` - Speedup relative to PyTorch
- `latency_scaling.png` - Scaling analysis (linear and log-log)
- `summary_report.txt` - Text summary of results

### 4. Profile Kernels

Basic profiling with PyTorch profiler:

```cmd
python python/profiling/profile_kernels.py --impl flash
```

Advanced profiling with NVIDIA Nsight Compute:

```cmd
ncu --set full --export profile_report python python/profiling/profile_kernels.py --impl flash
```

Then open `profile_report.ncu-rep` in Nsight Compute GUI.

## Usage Examples

### Python API

```python
import torch
from attention_cuda import attention_flash

# Create random inputs
batch_size, num_heads, seq_len, head_dim = 2, 8, 512, 64
Q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
K = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
V = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

# Run optimized attention
output = attention_flash(Q, K, V)

print(f"Output shape: {output.shape}")  # [2, 8, 512, 64]
```

### Module API (for nn.Module integration)

```python
import torch.nn as nn
from attention_cuda import AttentionFlash

class MyTransformerLayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.attention = AttentionFlash()
        # ... other layers
    
    def forward(self, Q, K, V):
        output = self.attention(Q, K, V)
        # ... rest of forward pass
        return output
```

### Benchmarking Custom Configuration

```python
import torch
import time
from attention_cuda import attention_flash
from attention_cuda.utils import generate_test_inputs, estimate_tflops

# Generate inputs
Q, K, V = generate_test_inputs(
    batch_size=4,
    num_heads=16,
    seq_len=1024,
    head_dim=64
)

# Warmup
for _ in range(10):
    _ = attention_flash(Q, K, V)

torch.cuda.synchronize()

# Benchmark
start = time.perf_counter()
for _ in range(100):
    output = attention_flash(Q, K, V)
torch.cuda.synchronize()
elapsed_ms = (time.perf_counter() - start) / 100 * 1000

# Calculate performance
tflops = estimate_tflops(4, 16, 1024, 64, elapsed_ms)
print(f"Latency: {elapsed_ms:.3f} ms")
print(f"Throughput: {tflops:.2f} TFLOPS")
```

## Troubleshooting

### Issue 1: "CUDA Available: False" in PyTorch

**Cause**: CPU-only PyTorch installed or CUDA version mismatch

**Solution**:
```cmd
# Uninstall current PyTorch
pip uninstall torch torchvision torchaudio

# Check your CUDA version
nvcc --version

# Reinstall PyTorch with matching CUDA version
# For CUDA 11.8:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Issue 2: "nvcc not found" during build

**Cause**: CUDA not in PATH

**Solution**: Add CUDA to system PATH
- Press `Win + R`, type `sysdm.cpl`, press Enter
- Go to "Advanced" â†’ "Environment Variables"
- In "System variables", find "Path", click "Edit"
- Add: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin`
- Restart Command Prompt

### Issue 3: "Microsoft Visual Studio not found"

**Cause**: Visual Studio C++ tools not installed

**Solution**: Install Visual Studio 2022 with "Desktop development with C++" workload

### Issue 4: Build fails with compiler errors

**Cause**: Version incompatibility

**Solution**: Ensure compatible versions:
- Python 3.10 or 3.11 (NOT 3.12)
- Visual Studio 2019 or 2022
- CUDA 11.8 or 12.1
- PyTorch 2.0+

### Issue 5: Out of memory errors

**Cause**: Sequence length too large for GPU memory

**Solution**: Reduce batch size or sequence length:
```cmd
python python/benchmarks/benchmark_attention.py --batch-size 1 --seq-lengths 128,256,512
```

### Issue 6: Incorrect results

**Cause**: GPU not being used

**Solution**: Verify GPU usage:
```cmd
python -c "import torch; t = torch.randn(100, 100, device='cuda'); print('GPU working!' if t.is_cuda else 'Using CPU')"
```

## Performance Expectations

### By GPU Model

| GPU Model | VRAM | Expected Performance | Use Case |
|-----------|------|---------------------|----------|
| GTX 1060 | 6GB | ~5 TFLOPS | Testing |
| RTX 2060 | 6GB | ~7 TFLOPS | Development |
| RTX 3060 | 12GB | ~13 TFLOPS | Good |
| RTX 3070 | 8GB | ~20 TFLOPS | Very Good |
| RTX 3080 | 10GB | ~30 TFLOPS | Excellent |
| RTX 4070 | 12GB | ~29 TFLOPS | Excellent |
| RTX 4080 | 16GB | ~49 TFLOPS | Outstanding |
| RTX 4090 | 24GB | ~83 TFLOPS | Best Consumer |

### Expected Speedups

| Sequence Length | PyTorch | Flash | Speedup |
|----------------|---------|-------|---------|
| 128 | 0.5 ms | 0.3 ms | 1.7x |
| 256 | 1.2 ms | 0.6 ms | 2.0x |
| 512 | 2.3 ms | 1.2 ms | 1.9x |
| 1024 | 9.1 ms | 4.5 ms | 2.0x |
| 2048 | 35.0 ms | 12.0 ms | 2.9x |

*Benchmarked on RTX 3080*

## Next Steps

### For Development
1. Modify CUDA kernels in `csrc/` directory
2. Rebuild: `python setup.py develop`
3. Test: `pytest tests/ -v`
4. Benchmark: `python python/benchmarks/benchmark_attention.py`

### For Profiling
1. Install NVIDIA Nsight Compute (free)
2. Profile: `ncu --set full python python/profiling/profile_kernels.py`
3. Analyze results in Nsight Compute GUI
4. Iterate on optimizations

### For Documentation
- Read [Design Decisions](docs/DESIGN.md) for architecture details
- Read [Optimization Techniques](docs/OPTIMIZATION.md) for optimization strategies
- Read [Profiling Guide](docs/PROFILING.md) for profiling instructions

## Common Commands

```cmd
# Run correctness tests
python python/benchmarks/compare_implementations.py

# Run full benchmarks
python python/benchmarks/benchmark_attention.py

# Generate visualizations
python python/benchmarks/visualize_results.py

# Profile with PyTorch
python python/profiling/profile_kernels.py

# Profile with Nsight Compute
ncu --set full python python/profiling/profile_kernels.py

# Run all unit tests
pytest tests/ -v

# Clean build
python setup.py clean --all
python setup.py develop
```

## Support

For issues or questions:
- Check [Troubleshooting](#troubleshooting) section above
- Review documentation in `docs/` directory
- Check CUDA and PyTorch installation

## Contributing

This is a demonstration project for NVIDIA interview purposes. Improvements welcome:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

Happy optimizing! ðŸš€
